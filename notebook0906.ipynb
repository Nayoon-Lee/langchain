{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model I/O : Input, Output\n",
    "    # 1. prompts, language models, output parsers\n",
    "# Retrieval : 외부 데이터에 접근해서 모델에게 제공.\n",
    "# Chain\n",
    "# Agents : chain이 필요한 도구를 선택할 수 있게 만듦.\n",
    "# Memory : 챗봇에 메모리를 넣는 것\n",
    "# Callbacks : 모델이 답변을 제공하기 전 실제로 모델이 무엇을 생각하고 있는지 중간단계를 알 수 있게 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "        Here is what I know:\n",
      "        Capital: Seoul\n",
      "        Language: Korean\n",
      "        Food: Kimchi and Bibimbap\n",
      "        Currency: South Korean Won"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\n        Here is what I know:\\n        Capital: Seoul\\n        Language: Korean\\n        Food: Kimchi and Bibimbap\\n        Currency: South Korean Won')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate # template을 string으로 받냐, message로 받냐의 차이\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Few shot templat : 모델이 어떻게 대답해야하는지에 대한 예제를 AI 모델에게 주는 과정\n",
    "# 세부적으로 모델에게 어떻게 해야하는지를 프롬프트로 전달하는 것보다 예제를 보여주는 것이 더 성공적일 것. -> 모델은 텍스트를 만들기 때문. \n",
    "# 고객지원 챗봇을 만든다면? : language model에게 어떻게 대응해야하는지는 대화기록을 데이터베이스에 가져와서 fewshot template으로 형식화 시키면 됨. \n",
    "\n",
    "# 1. 예제의 형식 지정하기 : 데이터베이스에서 가져올 수도 있기 때문에 예제의 형식을 지정해야함. \n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template=\"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# 2. 만든 예제 리스트를 fewshotPromptTemplate에 전달하여 어떻게 형식화할지 알려줌.\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix= \"Huamn: What do you know about {country}?\", # 형식화된 모든 예제 마지막에 나오는 내용. 따라서 사용자의 질문이 무엇인지 나오게 됨. \n",
    "    input_variables=['country'] \n",
    ")\n",
    "\n",
    "chain = prompt | chat \n",
    "chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        There are two Koreas - North Korea and South Korea\n",
      "        Capital of South Korea: Seoul\n",
      "        Language: Korean\n",
      "        Food: Kimchi and Bibimbap\n",
      "        Currency: South Korean Won"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        There are two Koreas - North Korea and South Korea\\n        Capital of South Korea: Seoul\\n        Language: Korean\\n        Food: Kimchi and Bibimbap\\n        Currency: South Korean Won')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate # template을 string으로 받냐, message로 받냐의 차이\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Few shot template : 모델이 어떻게 대답해야하는지에 대한 예제를 AI 모델에게 주는 과정\n",
    "# 세부적으로 모델에게 어떻게 해야하는지를 프롬프트로 전달하는 것보다 예제를 보여주는 것이 더 성공적일 것. -> 모델은 텍스트를 만들기 때문. \n",
    "# 고객지원 챗봇을 만든다면? : language model에게 어떻게 대응해야하는지는 대화기록을 데이터베이스에 가져와서 fewshot template으로 형식화 시키면 됨. \n",
    "\n",
    "# 1. 예제의 형식 지정하기 : 데이터베이스에서 가져올 수도 있기 때문에 예제의 형식을 지정해야함. \n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# 2. 이 나라에 대해 무엇을 알고 있어? 라고 물으면 ai가 \"answer\"처럼 답변했다고 속이게 하는 것.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. example 목록에 있는 각 예제의 형식을 지정해주는 과정. 이를 위해 위의 ChatPromptTemplate를 사용해 형식을 지정해줌.\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples, # suffix, input_variables가 필요 없음.\n",
    ")\n",
    "\n",
    "# 4. 이걸 ChatPromptTemplate를 사용하여 형식을 지정.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", \"You are a geography expert. You give short answers\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\"), # 이제 human이 새로운 질문을 할 것.\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat \n",
    "chain.invoke({\"country\":\"Korea\"})\n",
    "\n",
    "# AI 모델에게 전문가라고 말하면 답변이 길게 될 것. 채팅 인터페이스를 사용해서 더욱 길게 말하는 것도 있음.\n",
    "# You give short answers 라고 시스템 메시지를 설정할 수도 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about France?\\nAI: \\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Italy?\\nAI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuamn: What do you know about Brazil?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제를 선택하는 방법 : LengthBasedExampleSelector\n",
    "# 예제를 형식화하여 예제의 양이 얼마나 되는지 알려줌. 설정값에 따라 prompt에 알맞은 예제를 골라줌.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# 유저의 로그인 여부, 유저가 사용하는 언어에 따라 examples들을 허용할 수 있음. \n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=180,\n",
    ")\n",
    "\n",
    " # FewshotPromptTemplate는 리스트에 있는 모든 예제를 가지고 형식화하기 때문에 examples는 지워줌\n",
    "prompt = FewShotPromptTemplate( \n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix= \"Huamn: What do you know about {country}?\",\n",
    "    input_variables=['country'] \n",
    ")\n",
    "\n",
    "prompt.format(country='Brazil')\n",
    "# max_length=10, 어떤 예시도 선택되지 못함. -> 'Huamn: What do you know about Brazil?'\n",
    "# max_length=80, france가 선택됨\n",
    "# max_length=100, france, italy가 선택됨.\n",
    "\n",
    "# 이를 통해 LLM으로 예제 수를 제한할 수 있음. 비용 절약도 가능! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuamn: What do you know about Brazil?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제를 선택하는 방법 : LengthBasedExampleSelector\n",
    "# 예제를 형식화하여 예제의 양이 얼마나 되는지 알려줌. 설정값에 따라 prompt에 알맞은 예제를 골라줌.\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 예시로 example selector 만들기\n",
    "class RandomExampleSelctor(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)] # example은 list 형식\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# 유저의 로그인 여부, 유저가 사용하는 언어에 따라 examples들을 허용할 수 있음. \n",
    "example_selector = RandomExampleSelctor(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    " # FewshotPromptTemplate는 리스트에 있는 모든 예제를 가지고 형식화하기 때문에 examples는 지워줌\n",
    "prompt = FewShotPromptTemplate( \n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix= \"Huamn: What do you know about {country}?\",\n",
    "    input_variables=['country'] \n",
    ")\n",
    "\n",
    "prompt.format(country='Brazil')\n",
    "# max_length=10, 어떤 예시도 선택되지 못함. -> 'Huamn: What do you know about Brazil?'\n",
    "# max_length=80, france가 선택됨\n",
    "# max_length=100, france, italy가 선택됨.\n",
    "\n",
    "# 이를 통해 LLM으로 예제 수를 제한할 수 있음. 비용 절약도 가능! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialization and Composition\n",
    "# 직렬화, 불러오기 저장을 의미 / 작은 prompt를 가지고 이들을 모두 결합하는 것.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# .json, .yaml 파일로 prompt를 불러오고 포맷팅\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, nothin' beats the taste o' the sea! Arrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrr matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, nothin' beats the taste o' the sea! Arrr!\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PipelinePromptTemplate: 많은 prompt들의 memory등을 다 모으는 방법\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final,\n",
    "                                     pipeline_prompts=prompts,)\n",
    "\n",
    "# full_prompt.format(\n",
    "#     character=\"Pirate\",\n",
    "#     example_question=\"What is your location?\",\n",
    "#     example_answer=\"Arrrg! That is a secret!! Arg arg!!\",\n",
    "#     question=\"What is your fav food?\",\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke({\"character\":\"Pirate\",\n",
    "              \"example_question\":\"What is your location?\",\n",
    "              \"example_answer\":\"Arrrg! That is a secret!! Arg arg!!\",\n",
    "              \"question\":\"What is your fav food?\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough starts to form.\\n4. Use your hands to knead the dough until it becomes smooth and elastic. If the dough is too dry, you can add a little water to help it come together.\\n5. Once the dough is ready, cover it with a damp cloth and let it rest for about 30 minutes.\\n6. After resting, divide the dough into smaller portions and roll each portion out into a thin sheet using a rolling pin or pasta machine.\\n7. Cut the rolled-out dough into your desired pasta shape, such as fettuccine, spaghetti, or ravioli.\\n8. Cook the pasta in a large pot of boiling salted water for a few minutes until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings before serving.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching : llm 응답을 저장. 치팅 봇이 항상 똑같은 질문을 받는다면 계속 답변을 만들지 않고 이미 답변한 답을 캐싱을 이용하여 저장하여 재사용 가능!\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache()) # 모든 response가 메모리 저장될 것.\n",
    "# set_debug(True) # chain 작업할 때 log로 보여주어 편리함.\n",
    "set_llm_cache(SQLiteCache(\"cache.db\")) # 즉각 cache.db라는 데이터베이스가 생성됨.\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# in memory cache는 컴퓨터를 다시 시작하게 된다면 아까 질문했던 것을 다시 만들어야함.\n",
    "# 따라서 이럴 때는 데이터베이스에 llm 응답을 저장할 수 있음. \n",
    "\n",
    "chat.predict(\"How do you make Italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "- 1 cup of rice\n",
      "- 1 cup of water\n",
      "- 1 tablespoon of nuruk (fermentation starter)\n",
      "- 1 tablespoon of yeast\n",
      "\n",
      "Instructions:\n",
      "1. Rinse the rice under cold water until the water runs clear.\n",
      "2. In a large pot, combine the rice and water and bring to a boil. Reduce heat to low and simmer for 20 minutes, or until the rice is cooked.\n",
      "3. Allow the rice to cool to room temperature.\n",
      "4. In a separate bowl, mix the nuruk and yeast with a little bit of warm water to form a paste.\n",
      "5. Add the rice and nuruk/yeast mixture to a large glass jar or container and mix well.\n",
      "6. Cover the jar with a clean cloth and let it ferment in a warm, dark place for 1-2 weeks, stirring occasionally.\n",
      "7. After fermentation is complete, strain the liquid through a cheesecloth or fine mesh strainer to remove any solids.\n",
      "8. Transfer the liquid to a clean bottle and store in the refrigerator until ready to serve.\n",
      "\n",
      "Enjoy your homemade soju! Ingredients:\n",
      "- 4 cups all-purpose flour\n",
      "- 1 packet active dry yeast\n",
      "- 1 1/2 cups warm water\n",
      "- 2 tablespoons sugar\n",
      "- 2 teaspoons salt\n",
      "- 2 tablespoons olive oil\n",
      "\n",
      "Instructions:\n",
      "1. In a small bowl, dissolve the yeast and sugar in warm water. Let it sit for about 10 minutes until it becomes frothy.\n",
      "2. In a large mixing bowl, combine the flour and salt. Make a well in the center and pour in the yeast mixture and olive oil.\n",
      "3. Mix everything together until a dough forms. Knead the dough on a floured surface for about 10 minutes until it becomes smooth and elastic.\n",
      "4. Place the dough in a greased bowl, cover with a clean towel, and let it rise in a warm place for about 1 hour or until it doubles in size.\n",
      "5. Punch down the dough and shape it into a loaf. Place the loaf in a greased loaf pan, cover, and let it rise for another 30 minutes.\n",
      "6. Preheat the oven to 375°F (190°C). Bake the bread for about 30-35 minutes or until it is golden brown and sounds hollow when tapped on the bottom.\n",
      "7. Remove the bread from the oven and let it cool on a wire rack before slicing and serving. Enjoy your freshly baked bread! \n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용할 때 지출하는 비용을 아는 방법\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# with문과 print(usage) 사이에서 모델을 호출하면 됨. \n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading openai-chat LLM not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_llm\n\u001b[1;32m      5\u001b[0m \u001b[39m# chat = OpenAI(temperature=0.1,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#               max_tokens=450,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#               model=\"gpt-3.5-turbo-16k\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m chat \u001b[39m=\u001b[39m load_llm(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# chat.save(\"model.json\") # model.json이 저장됨.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m chat\n",
      "File \u001b[0;32m~/Documents/langchain/env/lib/python3.11/site-packages/langchain/llms/loading.py:44\u001b[0m, in \u001b[0;36mload_llm\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFile type must be json or yaml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Load the LLM from the config now.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m load_llm_from_config(config)\n",
      "File \u001b[0;32m~/Documents/langchain/env/lib/python3.11/site-packages/langchain/llms/loading.py:21\u001b[0m, in \u001b[0;36mload_llm_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     18\u001b[0m type_to_cls_dict \u001b[39m=\u001b[39m get_type_to_cls_dict()\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m config_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m type_to_cls_dict:\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mconfig_type\u001b[39m}\u001b[39;00m\u001b[39m LLM not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m llm_cls \u001b[39m=\u001b[39m type_to_cls_dict[config_type]()\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m llm_cls(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[0;31mValueError\u001b[0m: Loading openai-chat LLM not supported"
     ]
    }
   ],
   "source": [
    "# Serialization \n",
    "# 모델을 저장하고 불러오는 방법\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "# chat = OpenAI(temperature=0.1,\n",
    "#               max_tokens=450,\n",
    "#               model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "\n",
    "# chat.save(\"model.json\") # model.json이 저장됨.\n",
    "\n",
    "chat # 이제는 serialization을 더이상 지원을 안하는 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of salted boiling water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"How do you make Italian pasta\")\n",
    "# 바로 뚱 나오게 됨! LLM에게 물어보지 않고 캐싱을 이용해 답변을 재사용하여 비용을 절약한 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France is a country located in Western Europe, known for its rich history, culture, and cuisine. It is the largest country in the European Union by land area and the third-largest in Europe overall. The capital city is Paris, which is famous for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "\n",
      "France is known for its wine production, with regions such as Bordeaux, Burgundy, and Champagne producing some of the world's most renowned wines. French cuisine is also highly regarded, with dishes such as croissants, baguettes, escargot, and coq au vin being popular both in France and internationally.\n",
      "\n",
      "The country has a diverse landscape, ranging from the snow-capped Alps in the east to the sandy beaches of the French Riviera in the south. France is also home to numerous UNESCO World Heritage sites, including Mont-Saint-Michel, Versailles Palace, and the Loire Valley.\n",
      "\n",
      "French is the official language of France, and the country has a population of around 67 million people. France is a founding member of the European Union and is known for its strong cultural influence around the world, particularly in the fields of art, fashion, and literature."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"France is a country located in Western Europe, known for its rich history, culture, and cuisine. It is the largest country in the European Union by land area and the third-largest in Europe overall. The capital city is Paris, which is famous for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\\n\\nFrance is known for its wine production, with regions such as Bordeaux, Burgundy, and Champagne producing some of the world's most renowned wines. French cuisine is also highly regarded, with dishes such as croissants, baguettes, escargot, and coq au vin being popular both in France and internationally.\\n\\nThe country has a diverse landscape, ranging from the snow-capped Alps in the east to the sandy beaches of the French Riviera in the south. France is also home to numerous UNESCO World Heritage sites, including Mont-Saint-Michel, Versailles Palace, and the Loire Valley.\\n\\nFrench is the official language of France, and the country has a population of around 67 million people. France is a founding member of the European Union and is known for its strong cultural influence around the world, particularly in the fields of art, fashion, and literature.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"What do you know about France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
